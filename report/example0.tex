\pdfoutput=1

\documentclass{l4proj}

\usepackage{hyperref}
\hypersetup{
	colorlinks=false,
    citebordercolor={0 1 0},
    filebordercolor={1 1 0},
    linkbordercolor={1 0 0},
    urlbordercolor={0 1 1}
}
\usepackage[all]{hypcap}

\usepackage{chngcntr}
\counterwithout{figure}{chapter}
\counterwithout{table}{chapter}

\usepackage{listings}

\lstset{ %
basicstyle=\scriptsize\ttfamily,     % the size of the fonts that are used for the code
%basicstyle=\tiny,     % the size of the fonts that are used for the code
numbers=left,               % where to put the line-numbers
%numberstyle=\tiny,    % the size of the fonts that are used for the line-numbers
numberstyle=\scriptsize,    % the size of the fonts that are used for the line-numbers
numbersep=10pt,             % how far the line-numbers are from the code
frame=trBL,                 % adds a frame around the code
%tabsize=2,                  % sets default tabsize to 2 spaces
captionpos=b,               % sets the caption-position to bottom
breaklines=true,            % sets automatic line breaking
breakatwhitespace=false,    % sets if automatic breaks should only happen at whitespace
showstringspaces=false,     % Don't show underscores as space characters
frameround=fttt
}

\usepackage{caption}

\begin{document}
\title{Exploring the possibility of accelerating Hurricane Simulator using Apache Spark framework}
\author{Andrej Hoos}
%\date{March ???, 2016}
\maketitle

\begin{abstract}
The bestest abstract
\end{abstract}

\educationalconsent
%
%NOTE: if you include the educationalconsent (above) and your project is graded an A then
%      it may be entered in the CS Hall of Fame
%
\tableofcontents
%==============================================================================

%==============================================================================
%  Introduction
%==============================================================================
\chapter{Introduction}
\pagenumbering{arabic}

Simulating real world weather system is a difficult task, requiring a lot of computational power.
One such simulator is The Large Eddy Simulator for the Study of Urban Boundary-layer Flows (LES)
developed by  Hiromasa Nakayama and Haruyasu Nagai at the Japan Atomic Energy Agency
and Prof. Tetsuya Takemi at the Disaster Prevention Research Institute of Kyoto University\cite{les_analysis}\cite{les}.
It simulates meteorological systems in an urban environment with building resolution.
Originally LES is implemented in Fortran 77 with no explicit parallelization.

\section{Motivation \& Aim}

As multi core computational devices are now commonplace, parallelisation has become
a important consideration when implementing simulations. Therefore LES has been
ported to Fortran 95/OpenCL implementation by Wim Vanderbauwhed\cite{les_wim}. Using this implementation,
LES can be run parallelised on any OpenCL compatible computational device, which shows
significant improvements in speed. While these improvements can reduce the time needed to
run single simulation run or increase the size of the spatial domain of the simulation,
there are limits to both. One of these limits is number of computational units the device has,
limiting the extent of parallelization. The other is memory available on the device.

Using a network cluster of computers can, in theory, help overcome these limits to some extent.
However, splitting work onto multiple nodes in a cluster introduces a issue of communicating
important information over network. Specifically, in case of LES, data that is at the edge of
the spatial domain of each node needs to be exchanged in so called halo exchanges.

Recently, there has been a rise in a new computational frameworks focused on working in
distributed environment. One of these frameworks is Apache Spark which is based on
the MapReduce parallel execution paradigm.

The aim of this report is to explore the possibility of using the Apache Spark in Java to parallelize
LES, or any algorithm relying on halo exchanges in network distributed environment.

\section{Outline}

Further background will be described in \autoref{chap:background}, including information
on LES, Apache Spark and OpenCL implementation used. Implementation of halo exchanges in 
Apache Spark will be discussed in \autoref{chap:halos} and \autoref{chap:les_java} will
explain the process of porting LES code from Fortran to Java. Evaluation of the achieved
performance can be found in \autoref{chap:eval} and \autoref{chap:conclusion} contains
final remarks and notes about future work.

%==============================================================================
%  Background
%==============================================================================
\chapter{Background}
\label{chap:background}

\section{Large Eddy Simulator}

The Large Eddy Simulator for the Study of Urban Boundary-layer Flows developed by
Disaster Prevention Research Institute of Kyoto University is a high resolution
meteorological simulator designed to model turbulent flows over urban topologies. 
It achieves this by using mesoscale meteorological simulations and building resolving
large-eddy simulations. It also uses The Weather Research and Forecasting Model \footnote{\url{http://www.wrf-model.org}}
to compute the wind proile as an input for the simulation. The simulation is split into
multiple subroutines, however, the most important one is the Poisson Equation solver.
This is the most computationally intensive subroutine and it uses successive over relaxation to solve
a linear system of equations. Each time step of the simulation needs to execute following subroutines
in a sequence:

\begin{tabular}{ | l | l | }
  \hline	
  \textbf{velnw} & Update velocity for current time step \\
  \textbf{bondv1} & Calculate boundary conditions (initial wind profile, inflow, outflow) \\
  \textbf{velfg} & Calculate the body force \\
  \textbf{feedbf} & Calculation of building effects \\
  \textbf{les} & Calculation of viscosity terms (Smagorinsky model) \\
  \textbf{adam} & Adams-Bashforth time integration \\
  \textbf{press} & Solving of Poisson equation (SOR) \\
  \hline	
\end{tabular}

LES runs over spatial domain represent by a 3D grid. Size of 150x150x90 is used	for the most
of this report, but the size can be varied in x and y direction. By increasing the size of the
grid in x and y directions, LES can be run with higher resulution, or it can used 
to cover larger area. However doing this increases the time needed to complete the simulation
and more importantly there is a limit to how big the grid can be so that it fits inside the RAM.

\subsection{Paralellisation}

Using a cluster of computers, we can split the domain such that every node in the cluster runs
LES on a small portion of the domain. This allows to run LES on much larger domain than what would be
possible on a single computer. To run LES correctly in this scenario, each node needs access to the 
edge data belonging to each of its eight neighbours (Figure \ref{fig:neighbours}).

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{images/neighbours.png}
\caption{Node (x,y) needs access to the edges of all its neighbours}
\label{fig:neighbours}
\end{figure}

Message Passing Interface (MPI) \footnote{\url{http://www.mcs.anl.gov/research/projects/mpi/mpi-standard/mpi-report-2.0/mpi2-report.htm}} 
is considered to be most commonly used for communication in distributed memory environments.
Because of this, LES has already been parallelised using MPI by Gordon Reid as his MSci project at University of Glasgow\cite{les_mpi} and others\cite{les_palm}. Although MPI is considered standard and achieves good performance, it
isn't easy task to parallelise algorithms using MPI, especially for scientists with minimal
experience with Computing Science. Therefore it is important to investigate the possibilty
of using frameworks with higher level abstraction and that are easy to deploy onto a standard
network cluster.


\section{MapReduce parallel processing}

MapReduce is a programming model used mainly for processing large quantities of data
and is intended for network clusters\cite{map_reduce}. Program using this model consists of Map and Reduces
methods which are automatically paralellised. MapReduce was originally name of a technology by
Google, but is now used for the model in general. MapReduce libraries and frameworks are now 
available in many programming languages.

MapReduce model usually operates on key-value pairs and can be described by three main steps:

\begin{itemize}  
\item \textbf{Map}: \textbf{map()} function is applied to all of the pairs, creating a collection of new key-value pairs
\item \textbf{Shuffle}: the output data is redistributed accross network such that pairs with the same key are located on the same node
\item \textbf{Reduce}: the output data is processed by applying the \textbf{reduce()} function to the output pairs
\end{itemize}

Advantage of using MapReduce is that only map() and reduce() functions need to be written and
the shuffle step is handled automatically by the library or framework and is usually optimised. 
In addition, most MapReduce frameworks provided more data manipulation methods such as grouping or 
joining.

\subsection{Apache Spark}

Apache Spark\footnote{\url{http://spark.apache.org/}} is one of these frameworks and is available Java, Scala, Python and R. It claims
to be faster over alternatives such as Apache Hadoop, because it allows MapReduce to run in
memory as opposed to reading and writing inputs and outpus solely from disk. This, with combination
that is the increasing community and high-level abstraction makes this framework suitable for the purposes
of this project.	

TODO: Describe main points about how Spark works (master, worker, driver, RDDs...)

\section{OpenCL}

TODO: OpenCL background\footnote{\url{https://www.khronos.org/opencl/}}
\subsection{Aparapi}
TODO: Aparapi background
\footnote{\url{http://developer.amd.com/tools-and-sdks/opencl-zone/aparapi/}}
\footnote{\url{https://aparapi.github.io/}}

%==============================================================================
%  Halo exchanges in Spark
%==============================================================================
\chapter{Halo exchanges in Spark}
\label{chap:halos}

The main challenge of this project is to construct viable halo exchange algorithm, using 
data manipulation methods provided by Apache Spark. This has not been done before, although
one work\cite{seismic_spark} has used Spark to solve a stencil problem computation. In stencil
problems, the problem is divided into cells, and to compute one cell, information about its
neighbours are needed. However, only simple method using broadcast variables (covered later in the chapter)
was used in this work.

Multiple possible algorithms were considered and their implementation details, advantages
and drawbacks are discussed in this chapter.

\section{Game of Life as a proof of concept}
To test the proposed alorithms, a implemetation of Conway's Game of Life was used.
This was because of its simplicity and becasue it resembles the simulator in that
in needs data from neighbours to be transfered in order to be parallelised in 
distributed environment.

The Game of Life is a cellular automaton and is run on a 2D grid of cells. To compute
the transition of a single cell, state of each of its eight neigbours are needed.
The transitions are defined as:

\begin{itemize} 
\item Any live cell with fewer than two live neighbours dies.
\item Any live cell with two or three live neighbours lives on to the next generation.
\item Any live cell with more than three live neighbours dies.
\item Any dead cell with exactly three live neighbours becomes a live cell.
\end{itemize}

We can group the cells into same size rectangle shaped chunks and compute the transitions of each
chunk on a separate node of the cluster. However, now the nearest edges of neighbouring
chunks need to be exchanged before each transition, in order for the edge cells to know
the state of all of their neighbours. This is the same case in the LES and is therefore
suitable as a proof of concept.

\section{Broadcast variables}

The simplest way exchange halos is using broadcast variables. Spark allows to
create a broadcast variable in the driver program, which is the shipped to worker
nodes if needed. However the limitation is that only the driver program can create
broadcast variables (i.e. they cannot be created in the worker code) and therefore
the workers would need to send their edge data to the driver program, which would
contruct the appropriate halos and create a broadcast variable(TODO: diagram).

This means that the halos would be transferred to their destination always via
the node which hosts the driver program, and therefore the algorithm would do 
twice as much network transfers as the ideal case where the halos are transferred
to their destination directly. Therefore this approach was deemed unsuitable. 

\section{MapReduceByKey}

Another way is to use a combination of map and reduce steps (TODO: diagram). To represent the 
chunks, we use Spark Pair Collection of pairs of key and chunk (\texttt{JavaPairRDD<String, Chunk>}). The key represents
the coordinates of the chunk.

In the first stage we transform this collection into another collection which contains
the chunks from the original collection together with all neighbours using map
(\texttt{flatMapToPair}). The keys of the neighbours key value pairs are the keys of their respective
target chunk.

The second step then does reduce on this new collection using \texttt{reduceByKey}. 
The reducer function takes a chunk and a neighbour and reduces them into a chunk
with the neighbour added to it, which results in a collection of chunks which are
larger by one cell on each side.

The last step is a map step, which computes next transition on each chunk, returning
collection of transformed chunk which are the original size.

There are, however, some drawbacks to this approach. The first is, that we cannot
control the exact order in which these operations are executed. For example, Spark
might decide to execute the first map step on first two chunks (assuming there are more than two)
and run the reduce step on the results. Then, the first map step might be run on the rest
of the chunks. The second issue is that we cannot control, on which node is each step 
executed. This two issues result in all of the chunks and not only neighbours are
transferred over network, which is unacceptable for the purpouses of this project.

\section{MapGroupByKey}

To resolve one of the issues mentioned above, it is possible to replace the reduce
step with group step (TODO: diagram). This uses \texttt{groupByKey} to group chunk together with
neighbours that share the same key. The grouping function takes the chunk with
its neighbours and combines them into a single chunk, such that the result is the
same as after the reduce step in previous section.

Using this method the issue of the order of execution is solved, since the group
can be executed only after all of first map stage has been executed. The second issue
of not being able to control where is each stage executed still remains and therefore
the issue of transferring all chunk over network remains as well.

\section{MapCogroup}

Method described in this section was inspired by chapter 4 in \textit{Learning Spark}\cite{learning_spark}.
It uses custom partitioner and two separate collections for chunks and neighbours
to prevent Spark from moving chunks around the network. It uses combination
of map and cogroup steps.

The first map operates on the same key value pair collection as above. However,
now it transforms the collection into separate key value pair collection of neighbours,
instead of creating one mixed one as was done previously (TODO: diagram).

In second step, the \texttt{cogroup} transformation is very similar to the grouping transformation
from the MapGroupByKey method, with the difference that in this case it operates on two collections 
instead of one. The result is the same as in previous sections.

In addition, the collection containing chunks is partitioned using custom partitioner.
The partitioner takes the key, which represents coordinates and outputs index of node
on which the chunk should be physically present. Using this partitioner spark
shuffles only the other collection, which contains neighbour data, over network and
keeps the chunks collection in place.

\section{Caching and checkpointing}

Spark uses lazy execution when running programs stages. This means that transformation
on a collection are saved in its lineage are executed only after a call
that materialises the collection on the driver program is called on it. Examples of such calls are
\texttt{count} that returns the number of items in the collection, \texttt{collect}
which returns array of all items in the collection or \texttt{reduce}, which, as opposed
to \texttt{reduceByKey} reduces the collection into single item.

This introduces some issues. First issue is that often, when resolving stages that
do network shuffle, Spark executes the parent stages more than once. This is
considered standard Spark behaviour and is solved by caching the parent collection
using \texttt{cache} method, which also marks the collection to be explicitly reused
when executing, child stages. However, it was found that Spark keeps more cached collections
than necessary, and therefore workaround was devised. Using this workaround, first,
new collection is created, then cached and materialised using \texttt{count} and 
then \texttt{unpersist} is called on the old collection to remove it from the cache.

Another issue is lineage length. In iterative algorithms, such as the ones described 
in this report, the lineage that Spark keeps in order to be able to restore execution
in the case of failure can get quite long. This results in significant slowdown of
Spark task resolution and eventually leads to stack overflow errors. One possible
solution is to increase JVM maximum stack memory, but the standard practice is to
\texttt{chekpoint} collections. This serializes the collection to disk and forgets
its lineage.

%==============================================================================
%  LES in Java
%==============================================================================
\chapter{LES in Java}
\label{chap:les_java}

The second challenge this chapter deals with is reimplementing LES in Java.
To do this, the OpenCL host code written in Fortran needs to be rewritten 
Java. This code consists mainly of a \texttt{switch} construct, which executes
different routines in the OpenCL kernel. Next methods for initialising variables,
constructing and deconstructing halos need to be created. Finally, the newly 
written Java OpenCL host code needs to be integrated with Spark.

\section{JNA with Fortran}

LES works with about 40 different variables that store states of diferent systems,
wind velocity, pressure, halos and grid constants. These variables need to be initialised
before the ex ecution of the simulation. This process is covered in a substantial part of
the Fortran source code.

Since accessing the existing Fortran subroutines is a viable strategy
\footnote{\url{http://www.javaforge.com/wiki/66061}},
it was decided to write a simple Fortran subroutine, which groups 
all subroutines that initialise the needed variables. The Fortran code
is compiled into a shared library, and can be accessed from Java using
Java Native Access \footnote{\url{https://github.com/java-native-access/jna}}.

\section{Aparapi for Unconventional Cores}

To write the OpenCL host code Aparapi for Unconventional Cores (Aparapi-ucores)
\cite{aparapi_ucores}
\footnote{\url{https://gitlab.com/mora/aparapi-ucores}}
is used as an underlying library that facilitates communication
with the OpenCL kernel. It is a fork of Aparapi library that allows
running OpenCL code on unconventional cores such as FPGAs. In addition
it allows running OpenCL kernels from precompiled binaries.

Aparapi allows to write Java code which is then translated into OpenCL kernel code
and executed. However, since the LES kernel code is already written, this is 
counterproductive. It is impossible to disable this behaviour and therefore a 
workaround is neccessary. Although, with binary flow, which is provided by Aparapi-ucores
it is possible to execute compiled binary of the provided kernel instead of 
kernel generated by Aparapi, Aparapi will ecpect the same method signature as 
the one the generated kernel has. Therefore it is needed to provide Aparapi with
dummy Java code, that translates into correct OpenCL method signature.
This can be achieved by using all parameters in correct order, the parameters
need to have correct datatype and parameters that are supposed to be written to
by kernel need to be marked as such by assigning a value into them. Example dummy
code is in the listing below:

\begin{lstlisting}[language=Java]
  float[] p2;
  float[] uvw;
  float[] uvwsum;
  float[] fgh;

  @Override
  public void run() {
    float sum = p2[0] + uvw[0] + uvwsum[0] + fgh[0];
    uvw[0] = sum;
    uvwsum[0] = sum;
  }
\end{lstlisting}
The corresponding kernel method signature as generated by Aparapi:
\begin{lstlisting}[language=C]
__kernel void run(
   __global float *p2, 
   __global float *uvw, 
   __global float *uvwsum, 
   __global float *fgh,
   int passid
)
\end{lstlisting}
In the example above, four kernel parameters are used. All are arrays of floats.
Note that the order in which they are listed in the sum in the first line of the 
\texttt{run} method, is the same as the order of the paramters in the signature.
In addition the \texttt{uvw} and \texttt{uvwsum} variables were assigned into,
which marks them as kernel-writable inside Aparapi. Also, important to note,
is final parameter \texttt{passid}, which is being automatically inserted by Aparapi,
and contains the id of the current kernel execution.

Due to the nature, there are some modification that need to be done to the OpenCL
kernel, before compiling into a binary. First, the \texttt{passid} needs to be added to
the parameter list. Second, Aparapi does not support passing non-constant scalar fields
into the kernel as paramter. As a workaround to this, scalar fields can be represented
as simple single item 1D arrays. Finally the name of the kernel method is restricted to
the name \texttt{run}. As of writing of this report, however, this was found not to be true
and it is possible to use different names, but the written code does not reflect this.

By default, Aparapi copies all parameters to and from kernel before and after every kernel
execution. This is due to large number of parameters in LES undesirable. Therefore, for
iterative applications, it is recommend to enable explicit Aparapi mode, which then requires
the programmer to call \texttt{put} and \texttt{get} methods to explicitely write and read 
parameters to and from the kernel.

\subsection{Device specific OpenCL binaries}

Since OpenCL is supposed to be compatible with wide range of devices, the same OpenCL
code can be executed on any OpenCL device. This is not true, however, for the 
compiled binaries, which are device specific. This means that new binaries need to 
be created whenever the simulation is to be run on a new device. There is no
tool available that allows easily compile kernel into binaries, but OpenCL specification
describes a method that allows to retrieve the compiled binary code
\footnote{\url{https://www.khronos.org/registry/cl/sdk/1.0/docs/man/xhtml/clGetProgramInfo.html}}.
Therefore it was decide to create a simple compilation tool 
\footnote{\url{https://github.com/adikus/cl-compile}}
which allows the user to specify a \texttt{.cl} kernel file and device with which to
compile the kernel file.

\subsection{Memory alignment}

Aparapi-ucores contains functionality which copies the contents of the parameters
into a memory aligned location. This is done because some devices such as FPGAs
do not interact well with unaligned memory locations, and locations in JVM heap
might not be aligned. This functionality is currently experimental, and allocates 
new memory for every kernel-writeable parameter every kernel execution. In addition
it copies the content of parameters from the original JVM memory location to this new 
location and back every kernel execution for every parameter. This results in a significant 
slowdown in execution, which is undesirable.

This functionality can be turned of, since in theory, using unaligned parameters
should not pose any issues when runnning the kernel on CPUs and GPUs. However,
after switching this functionality it was found, that the kernel no longer runs
properly on Intel CPU. The reason remains, as of writing of this report, unknown,
and therefore it was decided to modify the underlying Aparapi-ucores C++ source code
to create a workaroad that deals with the excesive memory copies. The workaround
allocates the aligned memory location only once and copies the memory from or to it
only when the \texttt{get} or \texttt{put} Aparapi methods are called.

\section{Halo construction and deconstruction}

Since LES has already been parallelised using MPI\cite{les_mpi}, functionality 
for reading and writing halos is already implemented inside the OpenCL kernel code.
This format is a single 1D for each halo and therefore needs to be deconstructed 
before sending and constructed again after receiving. 

Since the spatial domain in this project is considered to be wrapped such that
bottom connects to top and left connects to right, this halo construction and 
deconstruction can be demostrated on a single node (TODO: diagram). 

\section{LES in Apache Spark}

As a final part of the project implementaion, working LES Aparapi implementation
needs to be integrated into Spark. MapCogroup described in \autoref{chap:halos}
is used as the halo exchange algorithm.

Each time step of the simulation is divided into 11 steps, with each step executing
OpenCL kernel once. The 8th step (\texttt{PRESS\_SOR}) is a exception. 
In this step the successive over relaxation is run, which requres multiple 
iterations of kernel executions. Normally the over relaxation is run until 
the simulation state converges, but in this project it was decided to use constant
number of 50 iterations.

Each kernel execution needs to be accompanied be a halo write beforehand 
and halo read afterwards, both of which are an extra kernel execution.
After each group of halo write, simulation step and halo read the appropriate halos
should be exchanged over network. In addition some stages do a reduction over 
the whole domain, which means that a reduction over the nodes needs to be done using Spark.

Table \ref{tab:les_stages} contains the summary of each stage.

\begin{center}
  \begin{tabular}{ | l | l | l | }
      \hline
      \# & Stage & Halo exchange, reduction \\
      \hline
      1 & \texttt{VELNW\_\_BONDV1\_INIT\_UVW} & \texttt{p}, \texttt{uvw} and \texttt{uvwum} halo exchange \\
      2 & \texttt{BONDV1\_CALC\_UOUT} & \texttt{uvw} halo exchange \\
      3 & \texttt{BONDV1\_CALC\_UVW} & \texttt{uvw} halo exchange \\
      4 & \texttt{VELFG\_\_FEEDBF\_\_LES\_CALC\_SM} & \texttt{uvw}, \texttt{uvwsum}, \texttt{fgh}, 
      \texttt{diu} and \texttt{sm} halo exchange \\
      5 & \texttt{LES\_BOUND\_SM} & \texttt{sm} halo exhange \\
      6 & \texttt{LES\_CALC\_VISC\_\_ADAM} & \texttt{fgh} and \texttt{fgh\_old} halo exchange \\
      7 & \texttt{PRESS\_RHSAV} & \texttt{rhs} and \texttt{fgh} halo exchange + reduction \\
      8 & \texttt{PRESS\_SOR} & 
      \begin{tabular}[t]{@{}l@{}}
        \texttt{p} halo exchange, there are three kernel executions for each SOR \\
         iteration, which means three halo exchanges per iteration \\
        + one reduction per iteration to determine the convergence value 
      \end{tabular} \\
      9 & \texttt{PRESS\_PAV} & \texttt{p} halo exchange + reduction \\
      10 & \texttt{PRESS\_ADJ} & \texttt{p} halo exchange \\
      11 & \texttt{PRESS\_BOUNDP} & \texttt{p} halo exchange \\
      \hline
  \end{tabular}
  \captionof{table}{Summary of halo exchanges and reductions for each LES stage in one time step}
  \label{tab:les_stages}
\end{center}

Spark collection that contains the kernel hosts consists of key value pairs.
The key is represented by an integer which has value between 0 and X*Y-1, where X and Y
are the size of the node grid. For example, if X is 2 and Y is 3, it means we intend to
run the simulation on cluster with 6 worker nodes, and each node is running a simulation
on a 150x150x90 size domain. The value in the key value pair is represented by a subclass
of Aparapi Kernel class.

The neighbours are represent by a Neighbour class which encapsulates the underlying data
array and the direction from which it came (N,S,W,E,NW,NE,SW,SE).

Finally, helper methods have been created for executing stage of simulation, exchaning halos
and reductions. This means that a execution of a single simalation stage can be written similarly to:

\begin{lstlisting}
// 7th stage - PRESS_RHSAV
kernels = executeKernelStep(kernels, States.PRESS_RHSAV);
kernels = HaloExchanger.exchangeHalos(kernels, "rhs,fgh", ip, jp, kp, X, Y);
kernels = divisionReduction(kernels);
\end{lstlisting}

\subsection{Lineage \& Checkpointing}

The fact that single timestep of the simulation needs to be represented by
large number of jobs (10 stages * 2 [execution + halo echange] + 50 SOR iterations * 3
* 2 = 320; this is just an approximation) means that lineage grows very quickly.
With the recommended rate of chekpointing, which is once per 100 jobs, three 
checkpoints are created every time step of the simulation. Since one time step
should not take longer than couple of seconds, this means that large quantity 
of data is serialized to disk over the run of program.

This can create problems when trying to excute the simulation in environment
with restricted disk space. Therefore another workaround was devised, which 
all previous checkpoints before creating a new one. Checkpoints in Spark are used
to recover from a crash of a node or a similar scenario. However in our case,
recovering is not as simple as deserializing a class from disk. Further work
would need to be made to initialise the OpenCL kernel and load the recoverd state 
of parameters into it, and was deemed to be out of the scope of this project.
Therefore the checkpoints are completely useless and can be deleted without worry.

%==============================================================================
%  Evaluation
%==============================================================================
\chapter{Evaluation}
\label{chap:eval}

TODO: describe the setup, architecture, tests done, results, one or two bar charts..., easy stuff, skipping for now

\section{Architecture}

\footnote{\url{http://ark.intel.com/products/75267/Intel-Xeon-Processor-E5-2640-v2-20M-Cache-2_00-GHz}}


\section{Baseline tests}

\section{Spark tests - increasing domain size}

%==============================================================================
%  Conclusion
%==============================================================================
\chapter{Conclusion}
\label{chap:conclusion}

\section{Future Work}

\section{Summary}

%%%%%%%%%%%%%%%%
%              %
%  APPENDICES  %
%              %
%%%%%%%%%%%%%%%%
\begin{appendices}

\chapter{Running the simulation}

The project files that are required to run the parallelised version of LES can be found on GitHub.
\footnote{\url{https://github.com/adikus/hurricaneProject}}

TODO: Proper instructions, should be mirrored on GH

\end{appendices}

%%%%%%%%%%%%%%%%%%%%
%   BIBLIOGRAPHY   %
%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{plain}
\bibliography{bib}	

%%%%%%%%%%%%%%%%%%%
%   ???????????   %
%%%%%%%%%%%%%%%%%%%
%\begin{verbatim}
%H. Nakayama, T. Takemi, H. Nagai, and E. Agency,
%“Coupling of WRF and building-resolving urban CFD
%models for analysis of strong winds over an urban
%area,” 3rd International Workshop on Nonhydrostatic
%Numerical Models, vol. 3, no. RIKEN AICS, Kobe,
%Japan, 2014.
%\end{verbatim}

\end{document}
